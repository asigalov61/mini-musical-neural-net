{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch's nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dictionary\n",
    "word_to_ix = {\"p22\": 0, \"p23\": 1, \"p42\": 2, \"endp22\": 3, \"wait2\": 4, \"wait3\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "vocab_size = len(word_to_ix)\n",
    "embedding_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of word to integer\n",
    "word_to_ix[\"p42\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer\n",
    "embeds = nn.Embedding(vocab_size, embedding_dim)  # 6 words in vocab, 5 dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2])\n"
     ]
    }
   ],
   "source": [
    "# convert text -> integer -> 1d-tensor\n",
    "example_tensor = torch.tensor([word_to_ix[\"p42\"]], dtype=torch.long)\n",
    "print(example_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5775, -1.7360,  0.2905, -1.3652,  1.5233]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# embed 1d-tensor into 5-dim vector\n",
    "example_embed = embeds(example_tensor)\n",
    "print(example_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0394, -1.2079, -0.5101,  0.2840, -0.2728],\n",
      "        [ 1.6345, -1.4403, -1.1477,  0.0996, -0.9257],\n",
      "        [-0.6820,  0.7039, -0.3863,  1.0630,  1.0672],\n",
      "        [ 0.4961, -1.4209,  1.1120, -0.8681, -0.3484],\n",
      "        [ 0.4165,  0.9182,  0.9482,  1.0565, -1.2355]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# example of embedding a batch of 5 words for a vocab size of 149\n",
    "\n",
    "test = torch.tensor([22, 23, 46, 52, 72])\n",
    "\n",
    "embeds = nn.Embedding(149, 5)  # 149 words in vocab, 5 dimensional embeddings\n",
    "\n",
    "test_embed = embeds(test)\n",
    "\n",
    "print(test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[-0.0394, -1.2079, -0.5101,  0.2840, -0.2728],\n",
    "        [ 1.6345, -1.4403, -1.1477,  0.0996, -0.9257],\n",
    "        [-0.6820,  0.7039, -0.3863,  1.0630,  1.0672],\n",
    "        [ 0.4961, -1.4209,  1.1120, -0.8681, -0.3484],\n",
    "        [ 0.4165,  0.9182,  0.9482,  1.0565, -1.2355]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0394,  1.6345, -0.682 ,  0.4961,  0.4165],\n",
       "       [-1.2079, -1.4403,  0.7039, -1.4209,  0.9182],\n",
       "       [-0.5101, -1.1477, -0.3863,  1.112 ,  0.9482],\n",
       "       [ 0.284 ,  0.0996,  1.063 , -0.8681,  1.0565],\n",
       "       [-0.2728, -0.9257,  1.0672, -0.3484, -1.2355]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0394,  1.6345, -0.682 ,  0.4961,  0.4165],\n",
       "       [-1.2079, -1.4403,  0.7039, -1.4209,  0.9182],\n",
       "       [-0.5101, -1.1477, -0.3863,  1.112 ,  0.9482],\n",
       "       [ 0.284 ,  0.0996,  1.063 , -0.8681,  1.0565],\n",
       "       [-0.2728, -0.9257,  1.0672, -0.3484, -1.2355]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.transpose(1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Embedding\n",
    "\n",
    "1. define vocab length\n",
    "2. takes in a 0D-tensor (i.e. 49, 23, 24)\n",
    "3. no need for one-hot encoding\n",
    "4. if input tensor([49,23,34])\n",
    "    - will output tensor of 3*5\n",
    "    - in general, will output tensor of shape input_length*embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do\n",
    "1. remove one-hot encoding\n",
    "2. input should be 1d tensor\n",
    "3. output is still the same?\n",
    "    - usually output should be an embedding vector as well\n",
    "    - then use arg.max to get token_id\n",
    "4. use jupyter notebook\n",
    "5. remove keras's categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questions\n",
    "\n",
    "1. should the output still be the same?\n",
    "    - usually output should be an embedding vector as well\n",
    "    - then use arg.max to get token_id\n",
    "2. what should the embedding dimension be?\n",
    "    - use hyperparameter optimization to get best number, eg. ranging from 50 to 1000\n",
    "    - A good rule of thumb is 4th root of the vocab_length, eg. 149^(1/4) = 3.5\n",
    "    - The typical number of dimensions is between 200â€“300.\n",
    "    - The number of dimensions does not greatly impact how distances in the word embedding space encode semantic relationships. You can pick a power of 32 (64, 128, 256) to speed up modeling training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
