{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch's nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dictionary\n",
    "word_to_ix = {\"p22\": 0, \"p23\": 1, \"p42\": 2, \"endp22\": 3, \"wait2\": 4, \"wait3\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "vocab_size = len(word_to_ix)\n",
    "embedding_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of word to integer\n",
    "word_to_ix[\"p42\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer\n",
    "embeds = nn.Embedding(vocab_size, embedding_dim)  # 6 words in vocab, 5 dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2])\n"
     ]
    }
   ],
   "source": [
    "# convert text -> integer -> 1d-tensor\n",
    "example_tensor = torch.tensor([word_to_ix[\"p42\"]], dtype=torch.long)\n",
    "print(example_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor = torch.tensor([1,2,3,4,5,1,2,3,4,5])\n",
    "example_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4236, -0.5190,  0.0436,  0.0023,  1.0424],\n",
      "        [-0.1578,  0.3750,  0.0773,  0.7948, -0.1578],\n",
      "        [ 0.0985,  1.0753,  0.5511, -0.6132, -0.0939],\n",
      "        [-0.4251,  0.9446,  2.0483,  0.0751, -0.1066],\n",
      "        [ 2.0693,  1.5477, -1.0879, -0.3237,  0.2905],\n",
      "        [ 1.4236, -0.5190,  0.0436,  0.0023,  1.0424],\n",
      "        [-0.1578,  0.3750,  0.0773,  0.7948, -0.1578],\n",
      "        [ 0.0985,  1.0753,  0.5511, -0.6132, -0.0939],\n",
      "        [-0.4251,  0.9446,  2.0483,  0.0751, -0.1066],\n",
      "        [ 2.0693,  1.5477, -1.0879, -0.3237,  0.2905]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# embed 1d-tensor into 5-dim vector\n",
    "example_embed = embeds(example_tensor)\n",
    "print(example_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6425,  0.0796,  0.4239, -0.1386, -0.7610],\n",
      "        [ 0.0021, -0.1807,  0.1980,  0.7447, -0.0712],\n",
      "        [ 0.0964,  0.0788,  0.1742, -1.0710,  1.4882],\n",
      "        [-0.4784,  0.8262,  0.7358, -1.5041,  0.8019],\n",
      "        [-0.0998,  0.3637, -0.4066,  1.2723, -1.0680]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# example of embedding a batch of 5 words for a vocab size of 149\n",
    "\n",
    "example_batch = torch.tensor([22, 23, 46, 52, 72])\n",
    "\n",
    "embeds = nn.Embedding(149, 5)  # 149 words in vocab, 5 dimensional embeddings\n",
    "\n",
    "test_embed = embeds(test)\n",
    "\n",
    "print(test_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Embedding\n",
    "\n",
    "1. define vocab length\n",
    "2. takes in a 0D-tensor (i.e. 49, 23, 24)\n",
    "3. no need for one-hot encoding\n",
    "4. if input tensor([49,23,34])\n",
    "    - will output tensor of 3*5\n",
    "    - in general, will output tensor of shape input_length*embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do\n",
    "1. remove one-hot encoding\n",
    "2. input should be 1d tensor\n",
    "3. output is still the same?\n",
    "    - usually output should be an embedding vector as well\n",
    "    - then use arg.max to get token_id\n",
    "4. use jupyter notebook\n",
    "5. remove keras's categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questions\n",
    "\n",
    "1. should the output still be the same?\n",
    "    - usually output should be an embedding vector as well\n",
    "    - then use arg.max to get token_id\n",
    "2. what should the embedding dimension be?\n",
    "    - use hyperparameter optimization to get best number, eg. ranging from 50 to 1000\n",
    "    - A good rule of thumb is 4th root of the vocab_length, eg. 149^(1/4) = 3.5\n",
    "    - The typical number of dimensions is between 200â€“300.\n",
    "    - The number of dimensions does not greatly impact how distances in the word embedding space encode semantic relationships. You can pick a power of 32 (64, 128, 256) to speed up modeling training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
